#  This AI Learned What Humans REALLY Want in Just 10 Words 

  Learn how researchers fine-tuned GPT-3 with human feedback to improve truthfulness, reduce toxicity, and better follow user instructions - in just 30 words.

 The InstructGPT paper focuses on training large language models to follow instructions with human feedback. The authors note that making language models larger doesn’t inherently make them better at following a user’s intent. Large models can generate outputs that are untruthful, toxic, or simply unhelpful.

    To address this issue, the authors fine-tune language models on a wide range of 

    tasks using human feedback. They start with a set of labeler-written prompts and responses,

    then collect a dataset of labeler demonstrations of the desired model behavior. They

    fine-tune GPT-3 using supervised learning and then use reinforcement learning from human

    feedback to further fine-tune the model. The resulting model, called InstructGPT, shows

    improvements in truthfulness and reductions in toxic output generation while having

    minimal performance regressions on public NLP datasets.

    The authors conclude that fine-tuning with human feedback is a promising direc-

    tion for aligning language models with human intent.

-----
Image prompts (TODO):
A researcher fine tuning a machine behind a computer screen
A dialog where feedback shapes understanding between human and AI
A book being written through iterative collaboration