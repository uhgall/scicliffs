#  LLMs Level Up Reasoning Skills With "Strategic Thinking" Makeover 

  Scientists give language models new strategic planning skills.

 # A New Way for Language Models to Reason Like Humans

Language models have shown impressive abilities to reason through problems and generate coherent responses. However, researchers have found that large language models (LLMs) still lack some important reasoning skills that humans excel at. Specifically, LLMs cannot simulate possible outcomes of actions in their "head" or strategically plan steps toward a goal over multiple turns - core abilities we use every day. 

Researchers at multiple universities recently proposed a novel framework called Reasoning via Planning (RAP) to help address these limitations. RAP builds upon the ideas that humans rely on internal "world models" and strategic planning abilities during complex reasoning tasks. The core idea of RAP is to augment language models with a world model and principled planning algorithms in order to reason in a more human-like fashion. 

In RAP, the researchers repurposed the LLM itself as both the world model and reasoning agent. The world model allows the LLM to simulate how the "world state", like variable values or block configurations, may change after proposed actions. Armed with this model, the LLM then uses a powerful planning algorithm called Monte Carlo Tree Search (MCTS) to efficiently explore many reasoning paths while balancing exploration vs exploitation. 

MCTS builds a "tree" of simulated reasoning steps over multiple iterations. It evaluates the potential of each step based on looking ahead at future outcomes using the world model. Rewards guide the model toward more promising paths. This strategic, long-term planning is a key difference from prior methods that generate reasoning traces from left to right without forethought.

The researchers demonstrated RAP can solve a variety of reasoning challenges, like multi-step math problems, logical inferences, and complex tasks like generating block rearrangement plans. RAP substantially outperforms other popular LLM reasoning frameworks on these tasks. It even allows a 33% relative improvement over the extremely powerful GPT-4 model on one benchmark!

Overall, RAP presents an exciting new direction for LLM reasoning that is believed to be closer to human-level strategic thinking and planning. By combining world models, rewards, and advanced planning algorithms, this framework holds promise for building general reasoning skills in powerful language models.
# Image prompts (TODO):


* A 3D chessboard with pieces rearranging themselves over multiple turns

* A branching tree growing and blossoming as more reasoning steps and outcomes are simulated

* Cogs and gears turning inside an open mechanical head as strategies and plans are formulated

# Errata:

  Here are some potential typos, inaccuracies, dubious claims, and issues I noticed in the paper:

1) Table 1 caption has a typo - should be "Results on Blocksworld" instead of "Result on Blocksworld" (missing s). 

2) Section 4.2 says self-consistency improves over CoT by 2%, but Table 2 shows an improvement of ~4%, so the claim of 2% is inaccurate. 

3) Section 4.3 compares accuracy on PrOntoQA to "CoT + SC" but that was not defined as a baseline. 

4) Some equations like (1) and (2) are not properly formatted as mathematical expressions. 

5) Algorithm 1 is missing formatting for comments, conditional statements, assignments, etc. to clearly distinguish code elements. 

6) References are not formatted consistently (some with authors, some without) and some formatting issues like volume numbers missing. 

7) Claims about improvements over GPT-4 should acknowledge the models/hardware used may not exactly match. 

8) "RAP-Aggregation" is introduced but never fully defined what it does beyond "aggregating reasoning outputs".

9) Limited discussion of limitations - does not fully address potential issues like exposure bias due to model-based planning vs real environment interactions.

10) Someworld modeling claims could be stronger if backed by empirical evaluation showing the model learns accurate transition dynamics for the tasks.
