#  LLMs Level Up With World Model Planning - Achieve 33% Gain on Complex Reasoning Tasks 

  Discover how a new LLM reasoning framework overcomes key limitations by building an internal world model to plan multistep actions, enabling powerful problem solving abilities.

 # Reasoning with Language Model is Planning with World Model

## Shibo Hao*♣ Yi Gu*♣ Haodi Ma Joshua Jiahua Hong ♣ Zhen Wang ♣ ♠
                                         Daisy Zhe Wang Zhiting Hu♣
3                              ♠       ♣UC San Diego, University of Florida
2                             {s5hao, yig025, jjhong, zhw085, zhh019}@ucsd.edutelligence
2                                        {ma.haodi, daisyw}@ufl.edu
y
a
M                                                 Abstract

2                     Large language models (LLMs) have shown remarkable reasoning capabilities,
                      especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-
L                     humans, such as generating action plans for executing tasks in a given environment,
C                     or performing complex math, logical, and commonsense reasoning. The deficiency
s                     stems from the key fact that LLMs lack an internal world model to predict the world
c                     state(e.g., environment status, intermediatevariablevalues) andsimulatelong-term
[                     outcomes of actions. This prevents LLMs fromperformingdeliberate planningakin
1                     future states and rewards, and iteratively refining existing reasoning steps. Toing
v                     overcomethelimitations, weproposeanewLLMreasoningframework, Reasoning
9                     via Planning (RAP). RAP repurposes the LLM as both a world model and a
                      reasoning agent, and incorporates a principled planning algorithm (based on Monto
4                     reasoning, the LLM (as agent) incrementally builds a reasoning tree under theing
.                     guidance of the LLM (as world model) and task-specific rewards, and obtains a
0                     high-reward reasoning path efficiently with a proper balance between exploration
3                     vs. exploitation. We apply RAP to a variety of challenging reasoning problems
:                     on these tasks demonstrate the superiority of RAP over various strong baselines,lts
i                     including CoT and least-to-most prompting with self-consistency. RAP on LLaMA-
                      33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation
r                     setting.
a
               1   Introduction

               Large language models (LLMs) have exhibited emergent reasoning abilities in a wide range of
               tasks [5, 10, 44, 2]. Recent approaches further boost their ability by prompting LLMs to generate
               (e.g., least-to-most prompting [66]). However, LLMs still face difficulties with tasks that humans
               find easy. For example, in creating action plans to move blocks to a target state, GPT-3 [5] achieves a
               success rate of only 1%, compared to 78% for humans [57]; these models also struggle when solving
               complex tasks that require multiple steps of math, logical, or commonsense reasoning [65, 22, 41, 6].
               Humans possess an internal world model, a mental representation of the environment [28, 27, 15],  
               whichenables humans tosimulateactions andtheir effects ontheworld’s statefor deliberateplanning
                  equal contribution

               Preprint. Under review.

-----
Image prompts (TODO):
1. A ball of yarn unwinding to reveal wires connecting different parts of a machine representing an internal world model.
2. Vines growing between blocks symbolizing connections forming during multi-step reasoning.
3. A maze seen from above with a path lighting up representing planning to reach a goal.