# request_hash generated by GlimRequest with AnthropicRequestDetails
{
  "max_tokens_to_sample": 2000,
  "model": "claude-2",
  "prompt": "\n\nWrite a short \"teaser\" inspiring curiosity about the article below that is designed to get people to click on it. \nIt should be 30 words or less.\n---\n Here is the article in markdown format:\n\n<file pathname=\"article.md\">\n# Researchers Make Progress in Getting AI to Reason More Like Humans\n\nResearchers at UC San Diego and the University of Florida have developed a new technique to improve reasoning capabilities in large language models (LLMs) like GPT-3 and LLaMA. Their work, [published on arXiv](https://arxiv.org/abs/2303.10261), enables LLMs to reason in a more deliberate, human-like manner as opposed to the purely instinctive approach used today. \n\n## The Limitations of Current LLMs\n\nLLMs like GPT-3 have shown impressive reasoning skills, especially when prompted to break down complex questions into intermediate reasoning steps (also known as \"chain of thought\" reasoning). However, they still struggle with tasks that seem trivial for humans. \n\nFor example, when asked to generate a 2-6 step plan to rearrange blocks into a target configuration, GPT-3 only succeeds 1% of the time - compared to 78% for humans. LLMs also have difficulty with math, logical or common sense reasoning that requires multiple steps. \n\nThe researchers identify several key deficiencies of current LLM reasoning:\n\n- **No internal world model**: LLMs cannot predict how the world state changes in response to actions. Humans rely on their mental model of the environment to simulate potential outcomes when planning.\n\n- **No reward mechanism**: There is no way to assess if a reasoning step is moving towards the desired solution. Humans consciously evaluate options.\n\n- **No exploration/exploitation balance**: LLMs cannot balance trying new options vs choosing the best current one. Humans iteratively refine plans.\n\n## Introducing Reasoning via Planning\n\nTo address these gaps, the researchers developed a framework called **Reasoning via Planning (RAP)**. RAP incorporates two key components into the LLM:\n\n- A **world model** that predicts state changes. The LLM is repurposed to simulate world states based on actions.\n\n- A **planning algorithm** that guides reasoning towards high reward states. They use Monte Carlo Tree Search, which balances exploration vs exploitation. \n\nDuring reasoning, the LLM builds a tree of potential action sequences. The world model predicts outcomes, propagated as rewards to guide tree expansion. After iterative refinement, high reward reasoning traces emerge.\n\nRAP transforms the instinctive reasoning of LLMs into a more deliberate, goal-oriented planning process similar to human thinking. The explicit modeling of world states and rewards enables strategic exploration of the reasoning space.\n\n## Significant Improvements Across Tasks\n\nThe researchers validated RAP on a diverse set of reasoning tasks:\n\n- **Plan generation**: For block rearrangement problems in 2-6 steps, RAP succeeded 64% of the time compared to near 0% for standard CoT prompting. LLaMA-33B with RAP even exceeded GPT-4 performance.\n\n- **Math reasoning**: On grade-school math word problems (GSM8K dataset), RAP with LLaMA-33B solved 48.6% correctly, substantially better than CoT (29.4%) and least-to-most prompting (25.5%).\n\n- **Logical reasoning**: RAP achieved 94.2% accuracy on proving facts using real-world rules, improving 14 percentage points over CoT.\n\nThe consistent significant gains over strong baselines demonstrate RAP's potential as a general reasoning framework applicable to diverse domains.\n\n## Towards More Human-like AI\n\nBy combining language model capabilities with planning, the Reasoning via Planning framework marks an innovative step towards achieving more human-level deliberative reasoning in AI systems. The researchers summarize that RAP \"enables LLMs to reason in a manner close to humans' conscious planning.\"\n\nThe flexibility to define different states, actions and rewards also opens up many possibilities for applying RAP. As LLM performance continues rapid improvement, integrating human-inspired reasoning and planning abilities will become increasingly important to realize their full potential.\n</file>\n---\nRespond only with the teaser, nothing else."
}