# request_hash generated by GlimRequest with AnthropicRequestDetails
{
  "max_tokens_to_sample": 2000,
  "model": "claude-2",
  "prompt": "\n\nWrite an article about the academic paper below:\n- in the style of a blog post on a top rated science blog on substack. \n- for an audience of first principles thinkers with a good general science background. \n- explain all acronyms and jargon. Use markdown formatting.\n- IMPORTANT: Write from the perspective of a reader of the academic paper; for example, use phrases like \"The researchers at [University] found that...\"\ninstead of \"We found that\".\n\n---\n                             Reasoning with Language Model is\n\n                                  Planning with World Model\n\n\n\n                       Shibo Hao‚àó‚ô£ Yi Gu‚àó‚ô£ Haodi Ma Joshua Jiahua Hong ‚ô£ Zhen Wang ‚ô£ ‚ô†\n                                         Daisy Zhe Wang Zhiting Hu‚ô£\n3                              ‚ô†       ‚ô£UC San Diego, University of Florida\n2                             {s5hao, yig025, jjhong, zhw085, zhh019}@ucsd.edutelligence\n2                                        {ma.haodi, daisyw}@ufl.edu\ny\na\nM                                                 Abstract\n\n2                     Large language models (LLMs) have shown remarkable reasoning capabilities,\n                      especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-\nL                     humans, such as generating action plans for executing tasks in a given environment,\nC                     or performing complex math, logical, and commonsense reasoning. The deficiency\ns                     stems from the key fact that LLMs lack an internal world model to predict the world\nc                     state(e.g., environment status, intermediatevariablevalues) andsimulatelong-term\n[                     outcomes of actions. This prevents LLMs fromperformingdeliberate planningakin\n1                     future states and rewards, and iteratively refining existing reasoning steps. Toing\nv                     overcomethelimitations, weproposeanewLLMreasoningframework, Reasoning\n9                     via Planning (RAP). RAP repurposes the LLM as both a world model and a\n                      reasoning agent, and incorporates a principled planning algorithm (based on Monto\n4                     reasoning, the LLM (as agent) incrementally builds a reasoning tree under theing\n.                     guidance of the LLM (as world model) and task-specific rewards, and obtains a\n0                     high-reward reasoning path efficiently with a proper balance between exploration\n3                     vs. exploitation. We apply RAP to a variety of challenging reasoning problems\n:                     on these tasks demonstrate the superiority of RAP over various strong baselines,lts\ni                     including CoT and least-to-most prompting with self-consistency. RAP on LLaMA-\n                      33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation\nr                     setting.\na\n               1   Introduction\n\n               Large language models (LLMs) have exhibited emergent reasoning abilities in a wide range of\n               tasks [5, 10, 44, 2]. Recent approaches further boost their ability by prompting LLMs to generate\n               (e.g., least-to-most prompting [66]). However, LLMs still face difficulties with tasks that humans\n               find easy. For example, in creating action plans to move blocks to a target state, GPT-3 [5] achieves a\n               success rate of only 1%, compared to 78% for humans [57]; these models also struggle when solving\n               complex tasks that require multiple steps of math, logical, or commonsense reasoning [65, 22, 41, 6].\n               Humans possess an internal world model, a mental representation of the environment [28, 27, 15],\n               whichenables humans tosimulateactions andtheir effects ontheworld‚Äôs statefor deliberateplanning\n                  equal contribution\n\n               Preprint. Under review.                                                                                     Chain-of-Thought\n\n\n\n\n                                         Language Model\n\n\n\n\n\n\n\n\n\n                                     World\n\n                                     Model\n\n\n\n\n\n\n                                              Language Model\n                                                                                 Reasoning via Planning (RAP)\n\n\n\nFigure 1: An overview of Reasoning via Planning (RAP). Compared with previous LLM reasoning\nmethods like Chain-of-Thought [59], we explicitly model the world state from a world model\n\n(repurposed from the language model), enabling us to leverage advanced planning algorithms to solve\n\nthe reasoning problems.\n\n\n                                                                                        Julie is reading‚Ä¶ She wants to read half of the remaining\n                                                                                        pages tomorrow. How many pages should she read?\n                                 Goal: orange on blue; yellow on orange\n\n                                                                                                               { }\n\n\n                                                                                 Q1: How many pages                   Q : How many pages\n              Pickup orange                                     Pickup blue      did Julie read today?                   has she read?\n                 (r = 0.6)                                        (r = 0.4)           (r = 0.7)                             (r = 0.5)\n                                                                                           Q1: How ‚Ä¶Today?          Q1: How ‚Ä¶ read?\n\n                                                                                           A1: 24                   A1: 30\n\n                                                                                                                 Q 2: How many pages\n        Stack it on orange              Stack it on blue                        Q2: How many                     has Julie read till now?\n             (r = 0.3)                    (r = 0.9)                            pages should she                       (r = 0.8)\n                                                                                read tomorrow?\n                                                                                   (r = 0.3)                 Q1: ‚Ä¶\n                  ‚Ä¶                                                                        ‚Ä¶                 Q2: How ‚Ä¶now?\n                                                                                                             A2: 36\n\n\n     ‚Ä¶\n                                                                                                           Q 1: How‚Ä¶ today?\n                                                                                                           A 1: 24\n                                                                                                           ‚Ä¶\n                                                                                                           Q T: How ... tomorrow?\n                                                                                                           A T: 42\n                                                                                                                (Answer: 42)\n\n   Figure 2: RAP for plan generation in Blocksworld (left) and math reasoning in GSM8K (right).\n\n\n\n\nduringcomplextasksof motor control, imagery, inference, anddecisionmaking[54, 55, 4, 49, 17, 33].\n\nFor example, to make an action plan towards a goal, planning with the world model involves exploring\nvarious alternative courses of actions, assessing the likely outcomes by rolling out possible future\n\nscenarios, and iteratively refining the plan based on the assessment [25, 14, 52, 19, 48, 21]. This is\n\nin stark contrast to the current LLM reasoning, which instinctively generates a reasoning trace in\nan autoregressive manner. In particular, we identify several key limitations of the current reasoning\n\nwith LLMs, including (1) the lack of an internal world model to simulate the state of the world (e.g.,\n\nthe configuration of blocks, the values of intermediate variables), which is the foundation of human\nplanning; (2) the absence of a reward mechanism to assess and guide the reasoning towards the\n\ndesired state; and due to both of these limitations, (3) the incapability of balancing exploration vs.\n\nexploitation to efficiently explore the vast reasoning space.\n\nTo address these limitations, this paper proposes a new framework, Reasoning via Planning (RAP),\n\nthat enables LLMs to reason in a manner close to humans‚Äô conscious planning. RAP augments\n\nthe LLM with a world model, and reasons with principled planning (specifically Monte Carlo Tree\nSearch, MCTS) to produce high-reward reasoning traces after efficient exploration (Figure 1). Notably,\n\nwe acquire the world model by repurposing the LLM itself with appropriate prompts. During the\n\nreasoning, the LLM strategically builds a reasoning tree by iteratively considering the most promising\n\n\n\n                                                                       2reasoning steps (actions) and using the world model (the same, repurposed LLM) to look ahead for\nfuture outcomes. The estimated future rewards are then backpropagated to update the LLM‚Äôs beliefs\nabout the current reasoning steps, guiding it to refine the reasoning by exploring better alternatives.\nOur MCTS-based planning effectively maintains a proper balance between exploration (of unvisited\nreasoning traces) and exploitation (of the best reasoning steps identified so far).\n\nWe show RAP is a general framework applicable to a diverse range of challenging problems and\nachieves substantial improvements over recent popular LLM reasoning methods. In Blocksworld\n[57] for 2/4/6-step plan generation, RAP achieves an average success rate of 64% while CoT fails\nalmost completely. Moreover, LLaMA-33B with RAP surpasses GPT-4 with CoT by 33% relative\nimprovement. In math reasoning (GSM8K [11]) and logical inference (PrOntoQA [47]), RAP\nalso consistently improves over strong baselines, including CoT, least-to-most prompting, and their\nself-consistency variants.\n\n\n\n2    Related Work\n\nReasoning with LLMs. In the realm of LLMs [22, 41, 6], reasoning typically entails decomposing\ncomplex questions into sequential intermediate steps (a.k.a. chains) before producing the final\nanswer, exemplified by Chain-of-Thought (CoT) prompting and its variants [43, 59, 32]. The basic\n\nCoT approaches, which generate chains all at once, can induce additional errors as the step count\nincreases. One line of improvement methods involves sampling multiple chains and choosing the\nbest answer via majority voting, such as Self-Consistency [58]. Another line of work focuses on\ndecomposition, aiming to tackle the problem by solving multiple simple subproblems. For instance,\nleast-to-most prompting [66] reduces the question into subquestions and answers them sequentially.\nMore relevantly, similar to our reward formulation, some recent works have explored self-evaluation\napproaches, which leverage LLMs themselves to provide feedback for intermediate steps and then\ncontinue the reasoning [60, 51, 45]. For example, Paul et al. [45] fine-tune a critic model to provide\nstructured feedback iteratively in each step, and Madaan et al. [38] directly reuse the same LLM to\ngenerate multi-aspect feedback and refine the previously generated output. Besides, aligned with\nour state formulation, Li et al. [34] incorporates latent ‚Äúsituations‚Äù into LLMs, referring to the state\nof entities from the context. Nevertheless, none of the above methods formally introduce the world\nmodel and instantiates the reward and state into a unified framework.\n\nSearch-guided Reasoning with LLMs. Most of CoT approaches discussed above are based on a\nlinear reasoning structure. Self-consistency built onto CoT decodes multiple chains parallelly, but it\nremains hard to explore the reasoning space sufficiently. Recent efforts have been made to investigate\nnon-linear reasoning structures by sampling more reasoning steps efficiently guided by some search\nalgorithms [30, 67, 63, 64]. For example, Jung et al. [30] generate a tree of explanations to enforce\nlogical consistency, and Xie et al. [63] adopt beam search to decode a better CoT reasoning chain.\nMore recently, CoRe [67] proposes to fine-tune both the reasoning step generator and verifier for\n\nsolving math word problems, also using MCTS for reasoning decoding. Concurrently to our work,\nYao et al. [64] apply heuristic-based approach, like depth-/breadth-first search, to search for better\nreasoning paths. Compared with these search-guided methods, RAP is a more principled framework\nthat combines world model and reward within advanced MCTS planning. The RAP formulation of\nLLM reasoning with state, action, and reward also presents a more general approach applicable to a\nwide range of reasoning problems.\n\nPlanning with LLMs. Planning, a central ability in intelligent agents, involves generating a series\nof actions to achieve a specific goal [40, 7]. Classical planning methods have been widely adopted\nin robots and embodied environments [9, 42, 8, 61, 26]. Recently, prompting LLMs to do planning\ndirecly has gained attention and shown potential [24, 23, 53, 13, 35]. SayCan [1], for instance,\ncombines LLMs with affordance functions to generate feasible plans. Moreover, based on LLMs‚Äô\npowerful programming ability [37, 29, 36], some recent works first translate natural language\ninstructions into the executable programming languages, such as Planning Domain Description\nLanguage (PDDL), and runs classical planning algorithms, such as LLM+P [36]. However, code-\nbased planning is constrained by its narrow domains and the predefined environment, while RAP can\nhandle open domain problems, including numerical and logical reasoning (see Section 4.2 and 4.3).\n\nWorld models and Planning. Traditional reinforcement learning (RL) heavily relies on interaction\nwith the environment (real world or simulators). To improve sample efficiency, previous research\n\n\n                                                  3attempts to learn a world model that predicts state transition, and directly learn a policy within the\nworld model [16, 17]. With latent imagination in a world model, an RL agent can be trained to solve\nlong-horizon tasks [18, 20]. Besides, the world model is also shown to be helpful to physical robot\nlearning [62]. Recent years have witnessed successful applications of planning algorithms in RL [50],\n\nsuch as AlphaZero [52], MuZero [48]. These algorithms are typically based on tree-structured search\nand are designed to effectively maintain the balance of exploration and exploitation. In this paper, we\nuse LLMs as world models and apply a planning algorithm to search for a reasoning path. Combining\nworld model and planning, our framework is similar to model predictive control [8]. Compared with\nprevious works, our framework uses general LLMs as the world model and can be adapted to a wide\n\nrange of open-domain reasoning tasks.\n\n\n3    Reasoning via Planning (RAP)\n\nIn this section, we present the Reasoning via Planning (RAP) framework that enables LLMs to\n\nstrategically plan a coherent reasoning trace for solving a wide range of reasoning tasks. We first\nbuild the world model by repurposing the LLM with prompting (Section 3.1). The world model\nserves as the foundation for deliberate planning, by allowing the LLM to plan ahead and seek out\nthe expected outcomes in the future. We then introduce the rewards for assessing each state during\nreasoning in Section 3.2. Guided by the world model and rewards, the planning with Monte Carlo\n\nTree Search (MCTS) efficiently explores the vast reasoning space and finds optimal reasoning traces\n(Section 3.3). Finally, when multiple promising reasoning traces are acquired during planning, we\nfurther introduce an aggregation method in Section 3.4 that yields an integrated result and further\nboosts the reasoning performance.\n\n\n3.1 Language Model as World Model\n\nIn general, a world model predicts the next state of the reasoning after applying an action to the\ncurrent state [17, 39]. RAP enables us to instantiate the general concepts of state and action in\n\ndifferent ways depending on the specific reasoning problems at hand. For example, in Blocksworld\n(Figure 2 left), it is natural to set a state to describe a configuration of blocks (with natural language),\nand an action to be a behavior of moving a block (e.g., ‚Äúpickup the orange block‚Äù). In a math\nreasoning problem (Figure 2 right), we use the state to represent the values of intermediate variables,\nand set an action to be a subquestion that drives the reasoning to derive new values (i.e., new state).\n\nAfter the definition of state and action, the reasoning process can thus be described as a Markov\ndecision process (MDP): given the current state s   t,t=0,1,...,Te.g., the initial sta0e s , the LLM (as a\nreasoning agent) generates an action following its generative distribution a ‚àº p(a|s ,c), where c is\n                                                                              t         t\na proper prompt (e.g., in-context demonstrations) to steer the LLM for action generation. The world\nmodel then predicts the next state st+1 of the reasoning. Specifically, we repurpose the same LLM to\nobtain a state transition distribution p(t+1 |st,at,c ), where c is another prompt to guide the LLM\nto generate a state. For instance, in Blocksworld, the LLM (as the world model) generates text s    t+1\nto describe the new configuration of blocks, given the previous state description s tnd the action a . t\n\nContinuing the process results in a reasoning trace, which consists of a sequence of interleaved states\nand actions (s 0a ,0 ,1..,a    T‚àí1 ,sT). This differs from the previous reasoning methods, such as\nChain-of-Thought [59], where the intermediate reasoning steps consist of only a sequence of actions,\n\ne.g., (a0= ‚Äúpickup red block‚Äù, a = ‚Äústack 1n yellow block‚Äù, ...) (see comparisons\nin Figure 1). Augmenting the reasoning with the (predicted) world states helps the LLM with a\nmore grounded and coherent inference. Note that the full reasoning trace is simulated by the LLM\nitself (as a reasoning agent with an internal world model) without interacting with the external real\nenvironment. This resembles humans contemplating a possible plan in their minds. The capability of\n\nsimulatingfuturestates, duetotheintroductionoftheworldmodel, allowsustoincorporateprincipled\nplanning algorithms to efficiently explore the vast reasoning space as described in Section 3.3.\n\n\n3.2   Reward Design\n\nDuring reasoning, we want to assess the feasibility and desirability of each reasoning step, and\nguide the reasoning based on the assessment (Section 3.3). The assessment of each reasoning step\n(i.e., applying an action a to the state s t is performed by a reward function r = rts ,a ) ‚ààtR.t\n\n\n                                                   4  ùë†\n\n  ùëé       orange      Pickup         orange     Pickup         orange      Pickup        Q        Q\n                       blue                      blue                       blue\n\n  ùë†\n\n  ùëé  Stack it    Stack it       Stack it    Stack it      Stack it     Stack it      Q        Q\n     on orange   on blue       on orange     on blue      on orange    on blue\n         ‚Ä¶                         ‚Ä¶                          ‚Ä¶                      ‚Ä¶\n  ùë†\n                                      Pickup    Pickup           Pickup   Pickup\n  ùëé                                   orange     red             orange     red            Q        Q\n\n  ùë†\n\n\n   ‚Ä¶                                                                                                     Q\n                                                                                                      reward\n\n  ùë†\n\n             (a) Selection              (b) Expansion              (c) Simulation       (d) Back-propagation\n\n\n     Figure 3: An illustration of the four phases in an iteration in MCTS planning (Section 3.3).\n\n\nSimilar to the state and action, the reward function can be specified in different ways to accommodate\n\nany knowledge or preferences about the reasoning problem of interest. Here we introduce several\ncommon rewards applicable to different tasks and shown to be effective in our experiments.\n\n\nLikelihood of the action. When an action is generated by the LLM conditioning on the current state,\nthe probability of the specific action reflects the LLM‚Äôs preference. We thus can incorporate the log\n\nprobability of the action as a reward.\n\nConfidence of the state. State prediction is nontrivial in some problems, e.g., in math reasoning\n\n(Figure 2, right), given an action (i.e., a subquestion), the world model predicts the next state by\nanswering the subquestion. We incorporate the confidence of the state (i.e., answers in this case) as a\n\nreward. Specifically, we draw multiple sample answers from the world model, and use the proportion\nof the most frequent answer as the confidence. A high confidence indicates a reliable reasoning step.\n\n\nSelf-evaluation by the LLM. We can also let the LLM criticize itself with the question ‚ÄúIs this\nreasoning step correct?‚Äù and use the next-word probability of the token ‚ÄúYes‚Äù as a reward.\n\nThe reward evaluates LLM‚Äôs own estimation of the correctness of reasoning. Similarly, we can get\nanother reward by prompting with the question ‚ÄúIs this reasoning step helpful?‚Äù, which is\n\na self-evaluation by LLM on the helpfulness of a reasoning step towards the target.\n\nTask-specific heuristics. We can also flexibly plug-in other diverse task-specific heuristics into the\n\nreward function. For example, in plan generation for Blocksworld, we compare the current predicted\nstate of blocks with the goal to calculate a reward (Section 4.1). The reward encourages the plan of\n\nmovements to actively pace towards the target.\n\n\n\n3.3   Planning with Monte Carlo Tree Search\n\nOnce equipped with the world model (Section 3.1) and rewards (Section 3.2), we can enable LLMs to\n\nreasonwithadvancedplanningalgorithms,whereweadoptMonteCarloTreeSearch(MCTS)[31,12],\na powerful planning algorithm that strategically explores the space of reasoning trees and strikes a\n\nproper balance between exploration and exploitation to find high-reward reasoning traces efficiently.\n\nMCTS builds a reasoning tree iteratively, where each node represents a state, and each edge represents\n\nan action and the transition from the current state to the next state after applying the action (Figure 1).\nTo guide the LLM agent to expand and explore the most promising nodes of the tree, the algorithm\n\nmaintains a state-action value function Q : S √ó A ‚Üí R, where Q(s,a) estimates the expected\n\nfuture reward of taking action a in state s. That is, we assess the potential of a node (or a reasoning\nstep) by looking ahead and anticipating the reward in future trajectories starting from this node.\n\nThis fundamentally differs from the current reasoning methods that generate a reasoning trace\nautoregressively from left to right without accounting for the future.\n\n\nMore specifically, as illustrated in Figure 3, the MCTS planning performs four operations in each\niteration to expand the tree and update Q values, i.e., selection, expansion, simulation, and back-\n\npropagation. The process continues until a specified computational budget (e.g., the number of\n\n\n\n                                                     5iterations) is reached, and the resulting reasoning traces are acquired from the tree, as we articulated\n\nlater. The psuedo-code of our MCTS planning is given in Algorithm 1 in the Appendix.\nSelection. The first phase selects a portion of the existing tree that is most promising for further\nexpansion in the next phase. Specifically, starting from the root node (i.e., initial state s ), at each\n                                                                                             0\nlevel of the tree, the algorithm selects a child node as the next node. The phase finishes when a\nleaf node of the current tree is reached. Figure 3(a) highlights the selected path in red. To balance\nbetween exploration (of less-visited nodes) and exploitation (of high-value nodes), we use the well-\nknown Upper Confidence bounds applied to Trees (UCT) algorithm [31] to select each child node.\nSpecifically, at node s, we select the action (which leads to a transition to a child node) in the tree by\nconsidering both the Q value (for exploitation) and uncertainty (for exploration):\n\n\n                           a = arg max        Q(s,a) + w        lnN(s)      ,                        (1)\n                                      a‚ààA(s)                  N(c(s,a))\n\n\nwhere N(s) is the number of times node s has been visited in previous iterations, and c(s,a) is the\nchild node of applying a in state s. Therefore, the less a child node was visited before (i.e., the more\nuncertain about this child node), the higher the second term in the equation. The weight w controls\nthe balance between exploration and exploitation.\n\nExpansion. This phase expands the tree by adding new child nodes to the leaf node selected above.\nSpecifically, given the state of the leaf node, we use the LLM (as agent) to sample d possible\nactions (e.g., subquestions in math reasoning), and then use the LLM (as world model) to predict the\nrespective next state, resulting in d child nodes. From the d nodes, we pick the node of largest local\nreward (Section 3.2) for simulation in the next phase. Note that if the leaf node selected above is a\nterminal (target) state already, we will skip expansion/simulation and jump to back-propagation.\n\nSimulation. This phase simulates the future situations of the current node using the world model, in\norder to estimate the expected future rewards (Q values). Specifically, starting from the current node\nas above, at each node s, we create an action following a roll-out policy and use the world model to\npredict the next state. The roll-out process continues until a terminal state if reached. There could be\ndifferent ways to define the roll-out policy (e.g., by adding different randomness). In our experiments,\n\nfor simplicity and reduced noises, we just follow the same process as in the expa‚Ä≤sion above, by\ngenerating d candidate actions and picking one of the largest local reward a = max r(s,a).aIn\npractice, as the roll-out process will evaluate the reward function for multiple nodes, for efficiency, we\ndiscard the computationally expensive components in r (for example, the reward from the confidence\nof state requires sampling the answer multiple times), and use the resulting light-weight reward\nfunction for selecting actions during simulation.\n\nBack-propagation. Once we reach a terminal state in the above phases, we obtain a reasoning path\nfrom the root node to the terminal node. We now back-propagate the rewards on the path to update\nthe Q value of each state-action pair along the path. That is, Q(s,a) is updated by aggregating the\nrewards in all future steps of node s. We may adopt the aggregation method according to the nature\nof different tasks and reward design, as discussed in Section 4.\n\nAs mentioned earlier, once a predetermined number of MCTS iterations is reached, we terminate\nthe algorithm and select final reasoning trace from the constructed tree. There could be various\nways for the selection. One approach is to start from the root node and iteratively choose the action\n\nwith the highest Q value until reaching a terminal. Alternatively, one can directly select the path\nfrom the iterations that yielded the highest reward, or opt to choose the leaf node (and the respective\nroot-to-leaf path) that has been visited the most. In practice, we observed that the second strategy\noften yields the best results.\n\n\n3.4 RAP-Aggregation: Aggregating Multiple Reasoning Outputs\n\nEnsemble-based methods, such as self-consistency CoT [58], can effectively improve performance\nby aggregating multiple valid reasoning traces. Therefore, for problems, such as math reasoning\n(Section 4.2) where only the final answer is required, RAP could produce multiple traces and answers\nfrom different MCTS iterations, which will be aggregated to produce the final answer. We refer to\nsuch a mechanism as RAP-Aggregation. Note that problems like plan generation or logical inference\n\nrequire a complete reasoning trace as output; thus, RAP-Aggregation will not be applied.\n\n\n                                                   6More importantly, there is a concern that some incorrect reasoning steps may appear in the early stage\nof multiple iterations, thus polluting the aggregation. As a result, we further devise a new weighting\n\nstrategy for aggregating candidate answers. Specifically, for each candidate answer, we accumulate\nthe reward of each reasoning step in the answer‚Äôs reasoning traces. We choose the answer with the\nhighest accumulative reward as the final aggregated answer.\n\n\n4    Experiments\n\nIn this section, we demonstrate the flexibility and effectiveness of our RAP framework by applying it\n\nto a wide range of problems, including plan generation in an embodied environment, mathematical\nreasoning for solving math word problems, and logical reasoning for verifying hypotheses. The\nsubsequent sections demonstrate how the world model formulation in RAP enables a versatile design\nof the state and action, catering to various reasoning contexts.\n\nWe primarily compare RAP with Chain-of-Thought (CoT) [59], and its variants like Least-to-Most\nprompting [66] as baselines. We also consider previous methods that ensemble reasoning paths from\nmultiple samples (also known as self-consistency [58]). Moreover, we compare RAP with GPT-4 [44]\nwhen computation resources allow. By default, we use the LLaMA-33B model [56] as the base LLM\nfor both our methods and baselines, and set the sampling temperature to 0.8.\n\n\n4.1 Plan Generation\n\nThe plan generation task aims to produce a sequence of actions to achieve a given goal, possibly with\nadditional constraints. The ability to generate plans is important for intelligent embodied agents,\ne.g. household robots [46]. This task has also been widely used to evaluate the reasoning ability of\nLLMs given their challenging requirements of long-horizon reasoning, e.g., Blocksworld is a classic\nproblem, where an agent is asked to rearrange the blocks into stacks in a particular order.\n\n\nTask setup. To explore the viability of the RAP framework for plan generation tasks, we adapt and\nevaluate RAP on the Blocksworld benchmark [50]. We define a state as the current orientation of the\nblocks and an action as an instruction that moves blocks. Specifically, an action is composed of one\nof the 4 verbs (i.e., TACK  , UNSTACK   , PUT , and P ICKUP  ) and manipulated objects. For the action\nspace, we generate the currently valid actions given the domain restrictions on actions and the current\n\norientation of the blocks. To transit between states, we take the current action and query the LLM to\npredict the state changes to the relevant blocks. We then update the current state by adding the new\nblock conditions and removing the conditions that are no longer true. Once a state has met all of the\nconditions listed in the goal or the depth limit of the tree is reached, we terminate the associated node.\n\nTo assess the quality of actions within this domain, we use two separate rewards. First, we prompt the\nLLM with some example test cases along with their solutions, and then calculate the log probability\nof the action given the current state (‚ÄúLikelihood of action‚Äù reward in Section 3.2), denoted as r .  1\nThis reward reflects the intuition of the LLM as the reasoning agent. It‚Äôs typically indicative when\nthere are few steps left to the goal, while not as reliable for a distant goal. Additionally, we compare\nthe new state after performing an action with the goal and provide a reward, r , sca2ing with the\nnumber of conditions met (‚ÄúTask-specific heuristics‚Äù reward). Specifically, when all the conditions\nare met, we assign a super large reward to make sure this plan will be selected as the solution.\n\n\nResults.    We use test cases from the Blocksworld dataset [57] and group them by solvable steps,\nresulting in 30 cases solvable with 2 steps, 57 cases with 4 steps, and 114 cases with 6 steps. There\nare at most 5 blocks in each test case. As the baseline method, we prompt the LLM with 4 test cases\nwith corresponding solutions, and ask it to generate a plan for a new question. This setting is the\nsame as one described in Valmeekam et al. [57], and we denote it as Chain-of-Thought (CoT) for\nbriefness. For RAP, the same prompt is shown to help LLMs calculate r .\n                                                                           1\nAs shown in Table 1, CoT with LLaMA-33B can only generate successful plans for a few 2-step\ncases, and completely fails on harder problems. RAP substantially improves over CoT by nearly\nsolving all problems within 4 steps, and a part of 6-step problems, achieving an average success rate\n                                                                                            6\nof 64%. It‚Äôs worth noting that the searching space of 6-step problems can be as large as 5 , while our\nalgorithm can find a successful plan 42% of the time within 20 iterations. Even more, our framework\n\n\n                                                   7                                           (10)          (20)\nTable 1: Results on Blocksworld. RAP           and RAP        refer to our method where the iteration\nnumber is set to 10 and 20, respectively. ‚Äúpass@10‚Äù is a relaxed metric, where 10 plans are sampled\nfor each test case, and the test case regarded as solved if at least one plan is successful. For all other\n\nsettings including RAP, only a single plan is evaluated.\n\n                                    Method      2-step    4-step    6-step\n\n                                        CoT      0.17      0.02      0.00\n                             CoT - pass@10       0.23      0.07      0.00\n\n                               CoT (GPT-4)       0.50      0.63      0.40\n                                         (10)\n                                    RAP          1.00      0.86      0.26\n                                    RAP  (20)    1.00      0.88      0.42\n\n\n\n      CoT      Goal: Red on blue; orange onRAPd                               Goal: Red on blue; orange on red\n                                           I have that, the red block is clear, the blue block is clear, the orange block is clear,\n                                           on the table, the blue block is on the table and the yellow block is on the table.s\n      unstack the orange                   Pick up the red block.\n      yellow blockn top of the\n                                           hand, the hand is holding the red block, the orange block is on top of the yellow in the\n                                           block, the blue block is on the table, and the yellow block is on the table.\n      stack the orange block               Stack the red block on top of the blue block.\n      on top of the red block              I have that, the orange block is clear, the red block is clear, the hand is empty, the\n                                           orange block is on top of the yellow block, the red block is on top of the blue block,\n                                           the blue block is on the table, and the yellow block is on the table.\n      pick up the yellow block             Unstack the orange block from on top of the yellow block.\n                                           I have that, the orange block is in the hand, the red block is clear, the yellow block\n                                           is clear, the hand is holding the orange block, the red block is on top of the blue\n      on top of the blue block             block, the blue block is on the table, and the yellow block is on the table.\n                                           Stack the orange block on top of the red block.\n                                           I have that, the orange block is clear, the yellow block is clear, the hand is empty,\n                                           the orange block is on top of the red block, the red block is on top of the blue\n                                           block, the blue block is on the table, and the yellow block is on the table.\n\n      Figure 4: Comparing reasoning traces in Blocksworld from CoT (left) and RAP (right).\n\n\nallows LLaMA-33B to outperform GPT-4 by 33% relative improvement [44], which is known to\n\nhave much stronger reasoning ability [6].\n\nWe further present a case study of comparing the reasoning paths from Cot and RAP. As illustrated\nin Figure 4, we find the improvement can be mainly attributed to the following reasons: (1) By\nmaintaining the world state during reasoning, RAP can recognize valid actions for the current state,\n\navoiding generating illegal plans. (2) RAP is capable of backtracking and trying out other solutions\nwhen the first intuition from the LLM doesn‚Äôt work. Specifically, CoT attempts to achieve the second\ngoal, i.e. ‚Äúorange on red‚Äù, and achieve that with the first two steps. However, accomplishing the\n\nsecond goal first would prevent the first goal from being satisfied. On the contrary, even though\nRAP makes the same mistakes in the first iterations, our framework drives the agent to explore\nother possible paths (as described in Section 3.3) and finally generate a successful plan. (3) When\n\ncalculating r t we can only feed the current state to the LLM and hide the history. E.g., in the case\nof Figure 4, to calculate the reward for a ,2the LLM is provided with a ‚Äúnew‚Äù test case, in which\ns2is the initial state. This significantly lowers the difficulties of the last few steps, and saves more\n\niterations for harder decisions of the first few steps.\n\n\n4.2 Math Reasoning\n\nTask setup.     Numerical reasoning tasks, such as GSM8k [11], often include a description and a\n\nfinal question. To arrive at the answer to the final question, it is necessary to undertake multi-step\nmathematical calculations based on the problem‚Äôs context. It is thus natural to decompose the final\nquestion into a sequence of smaller sub-questions (Figure 2, right). To adapt RAP, we define a state\n\nas the values of intermediate variables, while an action is to propose an incremental sub-question\nabout a new intermediate variable. The world model then responds to the sub-question using the\nintermediate variables and the problem description, adding the new intermediate variable value into\n\nthe next state. We combine the self-evaluation of helpfulness by LLM r   t,1and the confidence of state\nrt,2using weighted geometric mean r = rt    t,1‚àór 1‚àíŒ±  as the reward function. This reward encourages\n                                                  t,2\n\n\n                                                   8                                                     50\n\n   Table 2: Results on GSM8k. The super-\n                                                     45\n   scripts indicate the number of samples\n   or iterations.                                    c\n                                                     r0\n               Method       Accuracy (%)             c\n                                                     35\n     Chain-of-Thought            29.4\n               + SC (10)         46.8                                                     Method\n                                                     30                                  Least-to-most\n         Least-to-Most           25.5                                                    Chain-of-thoughts\n               + SC (10)         42.5                                                    RAP (aggr)\n                                                     25\n                RAP  (1)         40.0                     1    2   3    4    5    6   7    8    9   10\n                    (10)                                           Number of samples (iterations)\n               RAP               48.6\n                 + aggr          51.6             Figure 5: The performance of RAP and baselines\n                                                  on GSM-8K, with different numbers of sampled\n                                                  paths or iterations.\n\n\n\n\n\nmore relevant and useful sub-questions. To account for the impact of the reasoning path‚Äôs length on\nthe reward, we compute the Q value by using the maximum of average rewards in future steps.\n\n                          Q (s ,a ) =             max           avg(r ,...,r ).                         (2)\n                                t   t    st,at,t ,...lsl,al,l+1s      t       l\n\n\nAs a related work, Least-to-Most prompting [66] shares a similar idea to us in sub-question decompo-\n\nsition, but they generate sub-questions all at once. On the contrary, RAP considers each action a         t\nbased on the current state s t which enables more informed decisions.\n\n\nResults.    We evaluate our framework on GSM8k, a dataset of grade high school math problems.\nWe also evaluate the base model with CoT prompting [59], Least-to-Most prompting [66], and their\n\nself-consistency [58] variants, as the baselines. We use the same 4-shot examples demonstrations for\nboth our framework and the baselines.\n\nAs shown in Table 2, our RAP framework answers 48.8% of the problems correctly, outperforming\n                                                                                          2\nboth the Chain-of-Thought and the Least-to-Most prompting with self-consistency . Notably, this\nresult is achieved when RAP only selects only one reasoning trace based on the reward. The\nintroduction of RAP-Aggregate further improves the accuracy by ‚àº 3%. We also calculate the\n\naccuracy with different numbers of iterations in MCTS and self-consistency samples in baselines,\nas illustrated in Figure 5. We find that across all numbers of iterations/samples, RAP-Aggregation\noutperforms baselines consistently, which indicates that when only a few iterations/samples are\n\nallowed, our framework is significantly better at finding reliable reasoning paths with the guide of\nreward.\n\n\n4.3 Logical Reasoning\n\n\nTask setup.     A logical reasoning task (e.g. PrOntoQA [47]) typically provides a set of facts and\nlogical rules, and a model is required to verify if a hypothesis fact is true or false by applying the\n\nlogical rules to the given facts, as illustrated in Figure 6. These tasks not only require the correct\nfinal answer (true/false), but also a detailed proof demonstrating the result. To apply our framework,\nwe define the state as a fact we are focusing on, analogous to the human‚Äôs working memory [3] for\n\ninference. An action is defined as selecting a rule from the fact set. The world model performs a one-\nhop reasoning step to get a new fact as the next state. The reward is calculated with Self-evaluation\n(Section 3.2. Specifically, we prompt the LLM with a few examples with their labels to help it better\n\nunderstand the quality of reasoning steps. We use the average reward of future steps to update the Q\nfunction, the same as Equation (2) for GSM8k.\n\n    2\n    While Touvron et al. [56] reports LLaMA‚Äôs results on GSM8K, there are not sufficient details to reproduce\ntheir results. As our setting is different from theirs, e.g., prompt design, we do not directly compare our results.\n\n\n\n                                                     9      Rules: (1) Carnivores are carnivorous. (2) Animals are not unicellular.\n          (3) Carnivores are mammals. (4) Every cat is a feline.\n          (5) Each feline is a carnivores. (6) ‚Ä¶\n      Facts: Fae is a feline.\n      Hypothesis: True or false: Fae is unicellular.\n    ùë†                         Fae is a feline.\n\n    ùëé      (5) Each feline is a carnivo(4) Every cat is a felinTable 3: ProntoQA results.\n                   (r = 0.8)              (r = 0.1)\n    ùë†              Fae is a carnivore.Fae is a cat.      Method         Pred Acc     Proof Acc\n\n            (1) Carnivores are                           CoT               87.8         64.8\n    ùëé         carnivorous.      (3) Carnivores are mammalCoT + SC          89.8           -\n               (r = 0.8)             (r = 0.8)\n    ùë†              ‚Ä¶           Fae is a mammal.          RAP (Ours)        94.2         78.8\n\n     ‚Ä¶\n\n    ùë†                         Fae is a not unicellular.\n   Figure 6: RAP planning on a PrOntoQA ex-\n\n   ample.\n\nResults.   We assess the performance of our RAP framework on PrOntoQA [47]. We adopt their\nsettings of ‚Äútrue‚Äù ontology (using real-world knowledge), ‚Äúrandom‚Äù ordering of rules. We mix the\nexamples requiring 3, 4, and 5 reasoning hops in a correct proof to prevent LLM from memorizing\n\nwhen to finish the reasoning. We sample 500 examples from the generation script released by Saparov\nand He [47]. We compare both the prediction accuracy of the final answer and the accuracy of the\nentire proof. We do 20 iterations for MCTS and 20 samples for self-consistency in baselines.\n\nAs the results presented in Table 3, our framework achieves a correct answer rate of 94.2%and a proof\naccuracy of 78.8%, surpassing the CoT baseline by 14% proof accuracy and the self-consistency\nCoT baseline by 4.4% prediction accuracy. Such substantial improvements clearly demonstrate the\n\neffectiveness of RAP in solving logical reasoning problems in the PrOntoQA dataset. Also, as the\ncase study illustrated in Figure 6, RAP can effectively recognize when a reasoning chain comes to\na dead end, and propagate the signal back to earlier reasoning steps, with the planning algorithm\nallowing it to explore alternatives to the previous steps. The self-evaluation reward further helps\nRAP to recognize potential incorrect reasoning steps, encouraging the agent to avoid them in future\n\niterations.\n\n\n5    Conclusion\n\n\nIn this paper, we present Reasoning via Planning (RAP), a novel LLM reasoning framework that\nequips LLMs with an ability to reason akin to human-like strategic planning. By coupling the LLMs‚Äô\nreasoning capabilities with a world model and principled planning via Monte Carlo Tree Search, RAP\nbridges the gap between LLMs and human planning capabilities. Our framework, which repurposes\n\nthe LLMto act as both a world model and a reasoning agent, enables the LLMto simulate states of the\nworld and anticipate action outcomes, while achieving an effective balance between exploration and\nexploitation in the vast reasoning space. Extensive experiments on a variety of challenging reasoning\nproblems demonstrate RAP‚Äôs superiority over several contemporary CoT-based reasoning approaches,\nand even the advanced GPT-4 in certain settings. RAP‚Äôs flexibility in formulating rewards, states, and\n\nactions further proves its potential as a general framework for solving diverse reasoning tasks. We\nposit that RAP, with its innovative melding of planning and reasoning, has the potential to redefine the\nway we approach LLM reasoning - essentially forging a new pathway toward achieving human-level\nstrategic thinking and planning in artificial intelligence.\n\n\n\nReferences\n\n [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\n      Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\n      as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n\n [2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\n\n      Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\n      arXiv preprint arXiv:2305.10403, 2023.\n\n\n                                                  10 [3] Alan Baddeley. Working memory. Science, 255(5044):556‚Äì559, 1992.\n\n [4] Robert Eamon Briscoe. Mental imagery and the varieties of amodal perception. Pacific\n     Philosophical Quarterly, 92(2):153‚Äì173, 2011.\n\n [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\n     Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\n     few-shot learners. Advances in neural information processing systems, 33:1877‚Äì1901, 2020.\n\n [6] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\n     Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\n     intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n\n [7] Tom Bylander. The computational complexity of propositional strips planning. Artificial\n     Intelligence, 69(1-2):165‚Äì204, 1994.\n\n [8] Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer science &\n     business media, 2013.\n\n [9] Jaime Carbonell, Oren Etzioni, Yolanda Gil, Robert Joseph, Craig Knoblock, Steve Minton, and\n     Manuela Veloso. Prodigy: An integrated architecture for planning and learning. ACM SIGART\n     Bulletin, 2(4):51‚Äì55, 1991.\n\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\n     Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\n\n     Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n\n     Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\n     solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\n[12] R√©mi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In\n     Computers and Games: 5th International Conference, CG 2006, Turin, Italy, May 29-31, 2006.\n     Revised Papers 5, pages 72‚Äì83. Springer, 2007.\n\n[13] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with\n     large language models for object rearrangement. arXiv preprint arXiv:2303.06247, 2023.\n\n[14] Wojciech W Gasparski and Tufan Orel. Designology: Studies on Planning for Action, volume 1.\n     Transaction Publishers, 2014.\n\n[15] Dedre Gentner and Albert L Stevens. Mental models. Psychology Press, 2014.\n\n[16] David Ha and J√ºrgen Schmidhuber. Recurrent world models facilitate policy evolution. Ad-\n     vances in neural information processing systems, 31, 2018.\n\n[17] David Ha and J√ºrgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.\n\n[18] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:\n     Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\n\n[19] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and\n     James Davidson. Learning latent dynamics for planning frompixels. In International conference\n     on machine learning, pages 2555‚Äì2565. PMLR, 2019.\n\n[20] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\n     discrete world models. arXiv preprint arXiv:2010.02193, 2020.\n\n[21] MarkKHo, DavidAbel, CarlosGCorrea, Michael LLittman, JonathanDCohen, andThomasL\n     Griffiths. Control of mental representations in human planning. arXiv e-prints, pages arXiv‚Äì\n     2105, 2021.\n\n[22] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A\n     survey. arXiv preprint arXiv:2212.10403, 2022.\n\n\n                                                11[23] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\n     zero-shot planners: Extracting actionable knowledge for embodied agents. In International\n     Conference on Machine Learning, pages 9118‚Äì9147. PMLR, 2022.\n\n[24] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng,\n     Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied\n     reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.\n\n[25] Quentin JM Huys, Neir Eshel, Elizabeth O‚ÄôNions, Luke Sheridan, Peter Dayan, and Jonathan P\n\n     Roiser. Bonsai trees in your head: how the pavlovian system sculpts goal-directed choices by\n     pruning decision trees. PLoS computational biology, 8(3):e1002410, 2012.\n\n[26] Yu-qian Jiang, Shi-qi Zhang, Piyush Khandelwal, and Peter Stone. Task planning in robotics:\n     an empirical comparison of pddl-and asp-based systems. Frontiers of Information Technology\n     & Electronic Engineering, 20:363‚Äì373, 2019.\n\n[27] Philip N Johnson-Laird. Mental models and human reasoning. Proceedings of the National\n     Academy of Sciences, 107(43):18243‚Äì18250, 2010.\n\n[28] Philip Nicholas Johnson-Laird. Mental models: Towards a cognitive science of language,\n     inference, and consciousness. Number 6. Harvard University Press, 1983.\n\n[29] Ana Jojic, Zhen Wang, and Nebojsa Jojic. Gpt is becoming a turing machine: Here are some\n     ways to program it. arXiv preprint arXiv:2303.14310, 2023.\n\n[30] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le\n     Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive\n     explanations. arXiv preprint arXiv:2205.11822, 2022.\n\n[31] LeventeKocsisandCsabaSzepesv√°ri. Banditbasedmonte-carloplanning. InMachineLearning:\n     ECML 2006: 17th European Conference on Machine Learning Berlin, Germany, September\n     18-22, 2006 Proceedings 17, pages 282‚Äì293. Springer, 2006.\n\n[32] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\n     language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n\n[33] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.\n     Open Review, 62, 2022.\n\n[34] Belinda Z Li, Maxwell Nye, and Jacob Andreas. Language modeling with latent situations.\n     arXiv preprint arXiv:2212.10012, 2022.\n\n[35] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion:\n     From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.\n\n[36] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter\n     Stone. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv\n     preprint arXiv:2304.11477, 2023.\n\n[37] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apid-\n     ianaki, and Chris Callison-Burch.     Faithful chain-of-thought reasoning.   arXiv preprint\n\n     arXiv:2301.13379, 2023.\n[38] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\n\n     Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\n     with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n\n[39] Yutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina Precup, David Silver, Masashi Sugiyama,\n     Eiji Uchibe, and Jun Morimoto. Deep learning, reinforcement learning, and world models.\n     Neural Networks, 2022.\n\n[40] John McCarthy. Situations, actions, and causal laws. Technical report, STANFORD UNIV CA\n     DEPT OF COMPUTER SCIENCE, 1963.\n\n\n                                               12[41] Gr√©goire Mialon, Roberto Dess√¨, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\n     Roberta Raileanu, Baptiste Rozi√®re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al.\n     Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.\n\n[42] Dana S Nau, Tsz-Chiu Au, Okhtay Ilghami, Ugur Kuter, J William Murdock, Dan Wu, and\n     Fusun Yaman. Shop2: An htn planning system. Journal of artificial intelligence research, 20:\n     379‚Äì404, 2003.\n\n[43] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\n     David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show\n     your work: Scratchpads for intermediate computation with language models. arXiv preprint\n\n     arXiv:2112.00114, 2021.\n[44] OpenAI. Gpt-4 technical report, 2023.\n\n[45] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert\n     West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv\n\n     preprint arXiv:2304.01904, 2023.\n[46] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio\n     Torralba. Virtualhome: Simulating household activities via programs, 2018.\n\n[47] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal\n     analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.\n\n[48] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Si-\n     mon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering\n\n     atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604‚Äì609, 2020.\n[49] Jay Schulkin. Action, perception and the brain: Adaptation and cephalic expression. Springer,\n\n     2012.\n[50] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak\n     Pathak. Planning to explore via self-supervised world models. In International Conference on\n\n     Machine Learning, pages 8583‚Äì8592. PMLR, 2020.\n[51] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\n\n     dynamic memory and self-reflection. ArXiv, abs/2303.11366, 2023.\n[52] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\n\n     Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering\n     chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint\n     arXiv:1712.01815, 2017.\n\n[53] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay,\n     Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task\n     plans using large language models. arXiv preprint arXiv:2209.11302, 2022.\n\n[54] Edward C Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948.\n\n[55] Marc Toussaint. Learning a world model and planning with a self-organizing, dynamic neural\n     system. Advances in neural information processing systems, 16, 2003.\n\n[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\n     th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\n     and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[57] Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao\n     Kambhampati. On the planning abilities of large language models (a critical investigation with\n     a proposed benchmark). arXiv preprint arXiv:2302.06706, 2023.\n\n[58] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-\n     consistency improves chain of thought reasoning in language models.           arXiv preprint\n     arXiv:2203.11171, 2022.\n\n\n                                                13[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\n     Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\n     arXiv:2201.11903, 2022.\n\n[60] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and\n     Yejin Choi. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053,\n     2022.\n\n[61] Grady Williams, Paul Drews, Brian Goldfain, James M Rehg, and Evangelos A Theodorou.\n     Information-theoretic model predictive control: Theory and applications to autonomous driving.\n     IEEE Transactions on Robotics, 34(6):1603‚Äì1622, 2018.\n\n[62] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-\n     dreamer: World models for physical robot learning. In Conference on Robot Learning, pages\n\n     2226‚Äì2240. PMLR, 2023.\n[63] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe\n\n     Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint\n     arXiv:2305.00633, 2023.\n[64] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\n\n     Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. 2023.\n[65] Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, and\n\n     Asli Celikyilmaz. Alert: Adapting language models to reasoning tasks. arXiv preprint\n     arXiv:2212.08286, 2022.\n\n[66] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\n     Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables\n     complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.\n\n[67] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang.\n     Solving math word problem via cooperative reasoning induced language models. arXiv preprint\n     arXiv:2210.16257, 2022.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                               14Algorithm 1 RAP-MCTS(S ,p ,r ,p ,d,0,N,Œ∏)  Œ∏   œï\n\n      Require: Initial state s1, state transition probability function Œ∏ , reward function Œ∏ , action generator\n         pœï, number of generated actions d, depth limit L, number of roll-outs N, and exploration weight w\n\n         Initialize memory of actions A : S ‚Üí A, children c : S √ó A ‚Üí S and rewards r : S √ó A ‚Üí R\n         Initialize the state-action value function Q : S √ó A ‚Üí R and visit counter N : S ‚Üí N\n         for n ‚Üê 0,...,N ‚àí 1 do\n\n            for t ‚Üê 0,...,L ‚àí 1 do\n                if st‚àà A then                                                     ‚ñ∑ Expansion & Simulation\n\n                    for i ‚Üê 1,...,d do\n                        Sample a  (i) ‚àº p œïa | s t, s (i) ‚àº p Œ∏s ,t   (i), and r(i) ‚àº r Œ∏s ta  (i)\n                                  t           (i)     t+1     (i)     t(i)      t              (i)\n                        Update A(s ) t {a }   t   i=1, c(st,a t ‚Üê s    t+1 , and r(st,a t ‚Üê r  t\n                    end for\n\n                end if\n                a ‚Üê argmax               Q(s ,a) + w        ln N(st)                              ‚ñ∑ Selection\n                  t             a‚ààe(s t       t            N(c(st,a))\n                s     ‚Üê c(s ,a ), r ‚Üê r(s ,a ), N(s          ) ‚Üê N(s       ) + 1\n                  t+1        t   t   t        t   t       t+1          t+1\n                if atis an output action then break\n            end for\n\n            T ‚Üê the actual number of steps\n            for t ‚Üê T ‚àí 1,...,0 do                                                        ‚ñ∑ Back propagation\n                Update Q(s ,at) wtth {r ,r t   t+1,...,r l\n\n            end for\n         end for\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                         15\n---\nRespond only with the article itself, nothing else. \n"
}