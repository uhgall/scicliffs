# request_hash generated by GlimRequest with AnthropicRequestDetails
{
  "max_tokens_to_sample": 2000,
  "model": "claude-2",
  "prompt": "\n\nWrite a short \"teaser\" inspiring curiosity about the article below that is designed to get people to click on it. \nIt should be 30 words or less.\n---\n Here is a draft blog post summarizing the key findings of the paper \"Reasoning with Language Model is Planning with World Model\":\n\n# New Research Shows How Language Models Can Reason More Like Humans\n\nResearchers at UC San Diego, University of Florida, and Google Brain recently published an intriguing paper [\"Reasoning with Language Model is Planning with World Model\"](https://arxiv.org/abs/2303.05247) highlighting how large language models (LLMs) like GPT-3 still struggle with complex, multi-step reasoning compared to humans. The researchers propose a new framework called Reasoning via Planning (RAP) that enables LLMs to reason more like how humans consciously plan. \n\n## The Limitations of LLMs in Reasoning\n\nLLMs have shown impressive capabilities in language tasks, but they still face difficulties in complex reasoning that requires multiple steps of logical, mathematical, or commonsense reasoning. For example, when asked to create a 2-6 step plan to rearrange blocks to a target configuration, GPT-3 only succeeds 1% of the time, compared to 78% for humans. \n\nThe researchers identify several key deficiencies of current LLM reasoning:\n\n- LLMs lack an **internal world model** to simulate how the world state changes in response to actions, which is critical for planning. Humans leverage such mental models to imagine different scenarios and outcomes.\n\n- LLMs have no explicit **reward mechanism** to assess if a reasoning step is good or bad, unlike humans who have intrinsic preferences and goals.\n\n- Due to the above limitations, LLMs cannot effectively **balance exploration and exploitation** to efficiently search the vast reasoning space, unlike human planning.\n\n## Introducing Reasoning via Planning (RAP)\n\nTo address the gaps, the researchers propose the RAP framework that incorporates two key components into LLM reasoning:\n\n- A **world model** that predicts the next state given the current state and action. This is implemented by repurposing the LLM itself with appropriate prompting.\n\n- **Monte Carlo Tree Search (MCTS)**, a powerful planning algorithm that incrementally builds a reasoning tree to explore promising chains of reasoning. MCTS balances exploration vs. exploitation to efficiently search for high-reward reasoning paths.\n\nThe framework formulates the LLM reasoning process as a Markov Decision Process. At each step, the LLM generates an action based on the current state. The world model (repurposed LLM) then predicts the next state. By simulating future states, the LLM can anticipate outcomes of actions. MCTS guides this process by maintaining value estimates of each reasoning step.\n\nRAP allows flexible design of rewards to assess reasoning steps, such as self-evaluations by the LLM on the correctness or helpfulness of each step. RAP is shown to be broadly applicable to diverse reasoning tasks.\n\n## Significant Improvements Across Reasoning Tasks\n\nThe researchers demonstrate RAP on challenging problems including block rearrangement, mathematical reasoning, and logical inference.\n\nOn 2-6 step block rearrangement planning, RAP achieves 64% success compared to near 0% for baseline CoT prompting. RAP also surpasses GPT-4, with LLaMA-33B + RAP improving over GPT-4 + CoT by 33% relatively.\n\nOn the GSM8K mathematical reasoning dataset, RAP improves accuracy to 51.6% from 29.4% of baseline CoT prompting. RAP also substantially outperforms on logical reasoning in the PrOntoQA dataset.\n\n## Key Takeaways\n\nBy incorporating planning with a world model into LLM reasoning, RAP significantly improves reasoning capabilities on diverse complex tasks. The results indicate integrating human-like planning abilities can enable LLMs to better tackle challenges requiring multi-step inference. The RAP framework offers a promising new direction for developing stronger reasoning in language models.\n\nThe paper provides an insightful analysis of current limitations of LLM reasoning, and proposes an innovative approach to address them. RAP demonstrates how we can leverage the core strengths of LLMs, while compensating weaknesses through principled algorithms and framework designs.\n---\nRespond only with the teaser, nothing else."
}