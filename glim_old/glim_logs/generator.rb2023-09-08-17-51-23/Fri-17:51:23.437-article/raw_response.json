{
  "completion": " Here is the article in the requested format: \n\n# Researchers Use Planning with World Models to Enable Complex Reasoning in Language Models\n\nA team of researchers from UC San Diego, University of Florida, and Columbia University recently made an exciting breakthrough in improving the reasoning capabilities of large language models (LLMs). In a paper published on ArXiv, they introduced a new framework called Reasoning via Planning (RAP) that enables LLMs to reason more like humans by incorporating planning with an internal world model. \n\nLLMs like GPT-3 have shown remarkable progress in language understanding and generation. However, they still struggle with complex, multi-step reasoning required for math, logic, and planning problems. The key limitation is that LLMs lack an internal model of the world to simulate future states and outcomes of actions. \n\nTo address this, the researchers augmented LLMs with a world model by repurposing the LLM itself to simulate state transitions. The LLM now acts as both the reasoning agent trying to solve problems, and the world model predicting effects of actions. This is akin to how humans contemplate plans in their minds.\n\nThe researchers also added rewards to guide the LLM towards valid reasoning steps. The LLM reasons by incrementally building a tree of possible action sequences, using the world model to estimate future rewards. The rewards are then backpropagated to update the LLM's beliefs to refine its reasoning. \n\nThis allows the LLM to deliberately plan ahead and balance exploration versus exploitation, like humans and advanced AI systems. The planning is based on Monte Carlo Tree Search, which has shown success in game-playing agents like AlphaGo.\n\nThe researchers evaluated RAP on diverse reasoning tasks including block rearrangement, math word problems, and logical inference. It achieved substantially higher accuracy than popular methods like chain-of-thought prompting and variants across tasks. RAP also improved over GPT-4 in block planning despite using the much smaller LLaMA model.\n\nOverall, this work demonstrates how augmenting LLMs with a capacity for planning and simulated world models can unlock more sophisticated reasoning. It opens exciting avenues for developing AI agents that can plan, deduce, and think ahead like humans. The flexible framework also allows formulating different states, actions, and rewards for various reasoning domains.\n\nBy combining strengths of LLMs and planning, RAP represents a milestone towards achieving strategic reasoning and foresight in artificial intelligence. The researchers believe it has immense potential to redefine the frontiers of language model capabilities.",
  "stop_reason": "stop_sequence",
  "model": "claude-2.0",
  "stop": "\n\nHuman:",
  "log_id": "5703b8dbcc1c55751ed77c236482d1fdb39b2967ea0b4fb584d88747e91c9e05"
}