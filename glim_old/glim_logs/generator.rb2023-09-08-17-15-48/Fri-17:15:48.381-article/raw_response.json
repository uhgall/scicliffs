{
  "completion": " Here is an article in Markdown format summarizing the key points of the academic paper \"Reasoning with Language Model is Planning with World Model\":\n\n# New Approach Enables Language Models to Reason Like Humans\n\nResearchers at UC San Diego and the University of Florida have developed a new technique that allows large language models (LLMs) like GPT-3 to reason more like humans. \n\n## The Limitations of Current LLMs\n\nLLMs like GPT-3 have shown impressive reasoning abilities, especially when prompted to generate intermediate reasoning steps (e.g. chain-of-thought prompting). However, they still struggle with complex tasks that require multiple steps of reasoning, such as:\n\n- Generating action plans to execute tasks in a given environment \n- Performing complex mathematical or logical reasoning\n\nThe key deficiency is that LLMs lack an **internal world model** to simulate outcomes of actions. This prevents deliberative planning where the model considers future states and iteratively refines reasoning steps.\n\n## Introducing Reasoning via Planning (RAP)\n\nTo address these limitations, the researchers propose **Reasoning via Planning (RAP)**. RAP incorporates two key components into the LLM:\n\n- **World Model:** The LLM is repurposed to simulate states of the world after actions.\n- **Planning Algorithm:** Uses Monte Carlo Tree Search to explore reasoning trajectories and find high reward paths. \n\nThis enables the LLM to look ahead at potential outcomes, and strategically build reasoning chains with a balance of exploration vs exploitation.\n\n![RAP overview figure](path_to_figure/rap_overview.png)\n\n_Overview of the RAP framework (image source: paper)_\n\n## Evaluating RAP on Diverse Reasoning Tasks\n\nThe researchers evaluated RAP on challenging reasoning tasks:\n\n- **Plan generation** in Blocksworld: RAP achieved 64% success rate compared to near 0% for baseline CoT.\n- **Math reasoning** on GSM8k dataset: RAP improves accuracy from 29.4% (CoT) to 51.6%.\n- **Logical reasoning** on PrOntoQA: RAP gets 94.2% accuracy compared to 87.8% for CoT.\n\nRAP consistently outperformed strong baselines like chain-of-thought and self-consistency prompting. The improvements demonstrate RAP's ability to reason more strategically.\n\n## Conclusion\n\nBy incorporating planning with a world model, RAP represents an important step towards human-like reasoning for LLMs. The flexibility of RAP makes it a general framework that could enable more capable reasoning across diverse applications.\n\nOverall, RAP offers an exciting new direction to realize stronger artificial intelligence through integrating planning and reasoning.",
  "stop_reason": "stop_sequence",
  "model": "claude-2.0",
  "stop": "\n\nHuman:",
  "log_id": "3a5871d70b084e2899107c60bc018b6c5f6e93e42f0fefd0d0fcff15f78796ab"
}