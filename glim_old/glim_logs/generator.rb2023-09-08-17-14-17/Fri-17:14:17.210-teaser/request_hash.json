# request_hash generated by GlimRequest with AnthropicRequestDetails
{
  "max_tokens_to_sample": 2000,
  "model": "claude-instant-1",
  "prompt": "\n\n\n\nTODO \n\nfix this for xml\n\nSYSTEM MESSAGE: ALWAYS, when asked to generate source code or other text files, use the following format:\n<file pathname=\"path_to_file/hello.rb\">\nputs \"Hello from Line 1\"\nputs \"hello from Line 2\"\n</file>\nSo, the example above shows how you would include a file called \"hello.rb\" that belongs in the subdirectory \"path_to_file\" of the current directory. \nThe file would contain two \"puts\" statements. \nUse this for all text files you generate, not just source code.\n\n\n\n\nWrite a short teaser for the article below that is designed to get people to click on it. The teaser should be 30 words or less.\n\n---\n\n Here are the key points I gathered from the paper:\n\n- Large language models (LLMs) have shown strong reasoning abilities when prompted to generate intermediate steps, but lack an internal world model to simulate outcomes of actions over multiple steps. \n\n- Reasoning via Planning (RAP) addresses this by augmenting the LLM with a world model and principled planning. \n\n- The world model is formed by repurposing the LLM itself to predict state transitions given actions. States represent aspects like block configurations or variable values.\n\n- Planning uses Monte Carlo Tree Search (MCTS) to strategically build a reasoning tree guided by rewards and the world model. Rewards assess step quality.\n\n- The framework is flexible - states, actions, and rewards can be defined differently for tasks like plan generation, math reasoning, or logical inference.\n\n- Experiments show RAP outperforms baselines like Chain-of-Thought on tasks requiring multi-step reasoning, like more complex Blocksworld or math problems. \n\n- It enables the LLM to backtrack, explore alternatives, and anticipate outcomes rather than just generating steps sequentially.\n\n- By combining world models, rewards, and search-based planning, RAP represents a more human-like approach to strategic reasoning compared to past LLM methods.\n\nIn summary, the paper presents a novel framework that equips LLMs with human-like planning abilities by integrating world modeling, rewards, and principled search algorithms like MCTS. This enables more coherent, long-term strategic reasoning compared to prior chaining-based LLM approaches."
}