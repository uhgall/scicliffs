{
  "completion": " Here is an adaptation of the academic paper on Reasoning with Language Models is Planning with World Model as a blog post on a science blog:\n\n# Reasoning Like Humans: Using World Models and Planning to Solve Complex Problems\n\nTraditional language models like GPT-3 have shown impressive reasoning abilities, but still struggle with tasks that seem easy for humans. For example, generating multistep plans or solving multi-step math problems. \n\nAs AI researchers at [University], we've been exploring how to give language models human-like strategic thinking and planning skills. Our key insight is that humans don't just reason step-by-step - we mentally simulate possible futures using our \"world model\", a mental representation of the environment. \n\nThis allows humans to deliberately plan by considering different action sequences, anticipating outcomes, and refining plans over time. Language models currently lack this ability. They autoregressively generate reasoning traces without looking ahead.\n\nTo address this, we propose a new framework called **Reasoning via Planning (RAP)**. The core idea is to equip language models with two key human capabilities - a world model and principled planning algorithms.\n\n## Building a World Model\nRAP starts by prompting the language model to act as its own \"world model\". For example, in a blocks rearranging task, the world model internally represents block configurations. In math problems, it tracks intermediate variable values. \n\nThis allows the model to simulate how the \"world\" would change after hypothetical actions. For blocks, it predicts new configurations after moves. For math, it derives new variable values after answering sub-questions.\n\n## Planning with Monte Carlo Tree Search\nOnce equipped with a world model, RAP uses advanced planning techniques like Monte Carlo Tree Search (MCTS) to efficiently explore the vast reasoning space.\n\nMCTS builds a \"tree\" representing different reasoning paths. It searches this tree using a balance of exploration and exploitation to identify high-reward paths - just like how humans carefully consider alternatives before committing to a plan.\n\n## Evaluating Progress with Rewards\nRAP also introduces rewards to evaluate the quality of each reasoning step based on factors like the language model's own confidence, or how close intermediate results bring us to the final answer. \n\nRewards guide search by propagating information about future reward expectations back through the tree. This encourages refinement of earlier reasoning steps to ultimately find better solutions.\n\n## Experimental Results\nWe tested RAP on challenging problems in domains like block rearrangement planning, math problem solving, and logical reasoning. \n\nIt significantly outperformed previous techniques, solving 64% of complex block rearrangement tasks versus near 0% for standard methods. RAP also exceeded the performance of the state-of-the-art GPT-4 model on certain reasoning benchmarks.\n\n## Conclusion\nBy fusing principles from planning and reinforcement learning, RAP takes a major step towards imbuing language models with human-like strategic reasoning capabilities. The framework is quite general and flexible, applicable to diverse problem types. \n\nWe believe techniques like RAP will be key to developing artificial general intelligence that can tackle complex, multi-step challenges through deliberate thought and planning, just as humans do. Exciting future work includes testing RAP in more complex, sequential decision making tasks.",
  "stop_reason": "stop_sequence",
  "model": "claude-instant-1.2",
  "stop": "\n\nHuman:",
  "log_id": "465e8937b17b865429c9d99ef2eadb8cca5a2207d6c33ae1a6f32cc47c2c9ac0"
}